{"title":"Parameter Space Noise For Exploration","date":"2019-11-06T12:11:01.000Z","date_formatted":{"ll":"Nov 6, 2019","L":"11/06/2019","MM-DD":"11-06"},"link":"2019/11/06/Parameter Space Noise For Exploration","tags":["RL"],"updated":"2019-11-06T12:11:01.000Z","content":"<p>这篇论文于2018年1月31日发表于ICLR会议。</p>\n<h1 id=\"1-简介\">1 简介<a title=\"#1-简介\" href=\"#1-简介\"></a></h1>\n<p>深度强化学习方法在探索行为方面通常通过在动作空间中加入噪声。而另一种可选的方法则是通过直接在agent的参数中添加噪声。诸如进化策略之类的方法用到参数干扰，但会丢弃过程中所有的时间结构，且需要更多的样本。将参数噪声和传统RL方法结合，可以将两者的优点结合起来。这篇论文证明了 off-policy 和 on-policy 方法都从这种方法中得到了改进，实验对比的算法有： DQN, DDPG, 和TRPO。实验环境在高维离散动作环境，且为连续控制任务。</p>\n<p>探索依然是现代深度强化学习中的一个重要挑战，探索的主要目的是确保 agent 的动作不会提前收敛到一个局部最优解。</p>\n<h1 id=\"2-off-policy-methods\">2 Off-Policy Methods<a title=\"#2-off-policy-methods\" href=\"#2-off-policy-methods\"></a></h1>\n<h2 id=\"2.1-deep-q-networks(dqn)\">2.1 Deep Q-Networks(DQN)<a title=\"#2.1-deep-q-networks(dqn)\" href=\"#2.1-deep-q-networks(dqn)\"></a></h2>\n<p>DQN使用深度神经网络作为函数近似器去估计最优Q值函数，该等式符合Bellman最优方程。</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?Q%28s_t%2Ca_t%29%3Dr%28s_t%2Ca_t%29+%5Cgamma%5Cmax_%7Ba%27%5Cin%20A%7DQ%28s_%7Bt+1%7D%2Ca%27%29\" alt=\"Q(s_t,a_t)=r(s_t,a_t)+ammaax_{a'n A}Q(s_{t+1},a')\" loading=\"lazy\" class=\"φbp\"></p>\n<h2 id=\"2.2-deep-deterministic-policy--gradients(ddpg)\">2.2 Deep Deterministic Policy  Gradients(DDPG)<a title=\"#2.2-deep-deterministic-policy--gradients(ddpg)\" href=\"#2.2-deep-deterministic-policy--gradients(ddpg)\"></a></h2>\n<p>DDPG是一种 actor-critic 算法，适用于连续动作空间。和DQN相似，其 critic 使用 off-policy 数据和递归的Bellman最优方程去估计Q值函数。</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?Q%28s_t%2Ca_t%29%3Dr%28s_t%2Ca_t%29+%5Cgamma%20Q%28s_%7Bt+1%7D%2C%5Cpi_%5Ctheta%28s_%7Bt+1%7D%29%29\" alt=\"Q(s_t,a_t)=r(s_t,a_t)+amma Q(s_{t+1},i_heta(s_{t+1}))\" loading=\"lazy\" class=\"φbp\"></p>\n<h1 id=\"3-on-policy-methods\">3 On-Policy Methods<a title=\"#3-on-policy-methods\" href=\"#3-on-policy-methods\"></a></h1>\n<h2 id=\"3.1-trust-region-policy-optimization(trpo)\">3.1 Trust Region Policy Optimization(TRPO)<a title=\"#3.1-trust-region-policy-optimization(trpo)\" href=\"#3.1-trust-region-policy-optimization(trpo)\"></a></h2>\n<h1 id=\"4-parameter-space-noise-for-exploration\">4 Parameter Space Noise for Exploration<a title=\"#4-parameter-space-noise-for-exploration\" href=\"#4-parameter-space-noise-for-exploration\"></a></h1>\n<p>为了实现结构化探索，我们从一系列策略中采样，这些策略都是通过在策略的参数向量中加入高斯噪声而得到。</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Cstackrel%7B%5Csim%7D%7B%5Ctheta%7D%20%3D%20%5Cmathcal%7BN%7D%280%2C%5Csigma%5E2I%29\" alt=\"tackrel{im}{heta} = athcal{N}(0,igma^2I)\" loading=\"lazy\" class=\"φbp\"></p>\n<p>重要的是，在整个过程中，我们对策略的采样只发生在每个episode的开端。为了方便和可读性，我们标记这个被干扰的策略为</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Cstackrel%7B%5Csim%7D%7B%5Cpi%7D%3A%3D%5Cpi_%7B%5Cstackrel%7B%5Csim%7D%7B%5Ctheta%7D%7D\" alt=\"tackrel{im}{i}:=i_{tackrel{im}{heta}}\" loading=\"lazy\" class=\"φbp\"></p>\n<p>类似的，</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Cpi%3A%3D%5Cpi_%5Ctheta\" alt=\"i:=i_heta\" loading=\"lazy\" class=\"φbp\"></p>\n<h2 id=\"state-dependent-exploration\">State-dependent exploration<a title=\"#state-dependent-exploration\" href=\"#state-dependent-exploration\"></a></h2>\n<p>动作空间噪声和参数空间噪声之间有重要的差异。</p>\n<p>考虑连续动作空间的情况：</p>\n<p>当使用高斯动作噪声，动作将会被采样根据一些随机策略产生：</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?a_t%3D%5Cpi%28s_t%29+%5Cmathcal%7BN%7D%280%2C%5Csigma%5E2I%29\" alt=\"a_t=i(s_t)+athcal{N}(0,igma^2I)\" loading=\"lazy\" class=\"φbp\"></p>\n<p>因此，即便对于一个固定的状态 s， 我们也几乎能肯定的获得一个不同的动作，无论在过程中该状态是否会再次被采样。因为动作空间噪声是完全独立于当前状态，需要注意，这对于相联动作空间噪声同样适用。</p>\n<h2 id=\"perturbing-deep-neural-networks\">Perturbing deep neural networks<a title=\"#perturbing-deep-neural-networks\" href=\"#perturbing-deep-neural-networks\"></a></h2>\n<p>通过使用球形高斯噪声这种有意义的方法去干预深度神经网络的参数不是很明显，因为深度神经网络一般有数百万的参数和复杂的非线性交互。然而，2017年 Salimans等人提出了一种简单的网络再参数化将其实现。更确切的说，我们在被干预的层之间使用 layer normalization。由于这在同一层的不同激活之间normalizing，所以，即使不同的层对噪声的敏感度不同，也可以在所有层上使用相同的干扰尺度。</p>\n<h2 id=\"adaptive-noise-scaling\">Adaptive noise scaling<a title=\"#adaptive-noise-scaling\" href=\"#adaptive-noise-scaling\"></a></h2>\n<p>参数空间噪声要求我们取一个合适的标量 sigma, 这可能是有问题的，因为该标量将强烈的依赖于特定的网络架构，并可能随着时间的推移而变化，因为随着学习的进展，参数对噪声变得更加敏感。另外，虽然容易直观的掌握动作空间噪声的尺度，但是很难理解参数空间的尺度。</p>\n<p>我们提出了一个简单的方案，以一种简单而直接的方式解决上述所有的限制。这是通过随着时间来调整参数空间噪声的尺度。并将其与它引起的动作空间变化相关联来实现的。更具体地说，我们可以在动作空间中被干扰与非干扰策略之间定义一个距离度量。并根据它是否低于或者高于某个阈值来相应增加或者减少参数空间噪声。</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Csigma_%7Bk+1%7D%20%3D%20%5Cbegin%7Bcases%7D%20%5Calpha%5Csigma_k%20%26%20%5Ctext%7Bif%20%7D%20d%28%5Cpi%2C%5Cstackrel%7B%5Csim%7D%7B%5Cpi%7D%29%5Cleq%20%5Cdelta%2C%5C%5C%20%5Cfrac%7B1%7D%7B%5Csigma%7D%5Csigma_%7Bk%7D%26%20%5Ctext%7Bif%20%7D%20otherwise%2C%20%5Cend%7Bcases%7D\" alt=\"igma_{k+1} = egin{cases} lphaigma_k &amp; ext{if } d(i,tackrel{im}{i})eq elta, rac{1}{igma}igma_{k}&amp; ext{if } otherwise, nd{cases}\" loading=\"lazy\" class=\"φbp\"></p>\n<p>其中 <em>α</em> 是一个标量因子， <em>δ</em> 是一个阈值。</p>\n<h2 id=\"parameter-space-noise-for-off-policy-methods\">Parameter space noise for off-policy methods<a title=\"#parameter-space-noise-for-off-policy-methods\" href=\"#parameter-space-noise-for-off-policy-methods\"></a></h2>\n<p>在 off-policy 情况下，参数空间噪声可以直接应用，因为通过定义可以使用 off-policy 收集的数据。我们仅仅会干扰探索的策略，，并通过replay这些数据来训练非干扰网络。</p>\n<h2 id=\"parameter-space-noise-for-on-policy-methods\">Parameter space noise for on-policy methods<a title=\"#parameter-space-noise-for-on-policy-methods\" href=\"#parameter-space-noise-for-on-policy-methods\"></a></h2>\n<p>参数噪声可以合并在一个 on-policy 设置中，使用一个改动的策略梯度 （Rückstieß 等人于2008年提出）。</p>\n<p>策略梯度方法优化</p>\n<p><img src=\"http://www.sciweavers.org/upload/Tex2Img_1573109616/render.png\" alt=\"img\" loading=\"lazy\" class=\"φbp\"></p>\n<p>给一个随机策略</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Cpi_%5Ctheta%28a%7Cs%29%2C%5Ctheta%5Csim%5Cmathcal%7BN%7D%28%5Ctheta%2C%5CSigma%29\" alt=\"i_heta(a|s),hetaimathcal{N}(heta,igma)\" loading=\"lazy\" class=\"φbp\"></p>\n<p>期望的回报可以用似然比和再参数化技巧来扩展(Kingma &amp; Welling, 2013)。</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Cbigtriangledown_%7B%5Cphi%2C%5CSigma%7D%5Cmathbb%7BE%7D_r%5BR%28%5Ctau%29%5D%5Capprox%5Cfrac%7B1%7D%7BN%7D%5Csum_%7B%5Cepsilon%5Ei%2C%5Ctau%5Ei%7D%5Cleft%5B%5Clarge%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D%5Cbigtriangledown_%7B%5Cphi%2C%5CSigma%7Dlog_%7B%5Cpi%7D%28a_t%7Cs_t%3B%20%5Cphi+%5Cepsilon%5Ei%5CSigma%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%29R_t%28%5Ctau%5Ei%29%20%5Cright%5D\" alt=\"igtriangledown_{hi,igma}athbb{E}r[R(au)]pproxrac{1}{N}um{psilon^i,au^i}eft[argeum_{t=0}^{T-1}igtriangledown_{hi,igma}log_{i}(a_t|s_t; hi+psilon^iigma^{rac{1}{2}})R_t(au^i) ight]\" loading=\"lazy\" class=\"φbp\"></p>\n","prev":{"title":"Algorithm: SumTree practice","link":"2019/11/08/Sumtree algorithm exercise"},"next":{"title":"Big V: vscode+anaconda","link":"2019/11/06/vscode+anaconda"},"plink":"http://thswind.github.io/2019/11/06/Parameter Space Noise For Exploration/","toc":[{"id":"1-简介","title":"1 简介","index":"1"},{"id":"2-off-policy-methods","title":"2 Off-Policy Methods","index":"2","children":[{"id":"2.1-deep-q-networks(dqn)","title":"2.1 Deep Q-Networks(DQN)","index":"2.1"},{"id":"2.2-deep-deterministic-policy--gradients(ddpg)","title":"2.2 Deep Deterministic Policy  Gradients(DDPG)","index":"2.2"}]},{"id":"3-on-policy-methods","title":"3 On-Policy Methods","index":"3","children":[{"id":"3.1-trust-region-policy-optimization(trpo)","title":"3.1 Trust Region Policy Optimization(TRPO)","index":"3.1"}]},{"id":"4-parameter-space-noise-for-exploration","title":"4 Parameter Space Noise for Exploration","index":"4","children":[{"id":"state-dependent-exploration","title":"State-dependent exploration","index":"4.1"},{"id":"perturbing-deep-neural-networks","title":"Perturbing deep neural networks","index":"4.2"},{"id":"adaptive-noise-scaling","title":"Adaptive noise scaling","index":"4.3"},{"id":"parameter-space-noise-for-off-policy-methods","title":"Parameter space noise for off-policy methods","index":"4.4"},{"id":"parameter-space-noise-for-on-policy-methods","title":"Parameter space noise for on-policy methods","index":"4.5"}]}],"reading_time":"1620 words in 11 min"}