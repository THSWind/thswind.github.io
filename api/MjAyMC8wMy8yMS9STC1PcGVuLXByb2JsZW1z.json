{"title":"RL: Open problems","date":"2020-03-21T04:36:18.000Z","date_formatted":{"ll":"Mar 21, 2020","L":"03/21/2020","MM-DD":"03-21"},"link":"2020/03/21/RL-Open-problems","tags":["RL"],"updated":"2020-03-21T04:36:18.000Z","content":"<p>There is a list of open questions related to exploration in reinforcement learning.</p>\n<!-- block -->\n<p>Below is a list of open questions related to exploration in reinforcement learning.</p>\n<ol>\n<li>How to determine whether an agent is doing good, intelligent exploration?</li>\n<li>How can we determine when exploration is the bottleneck to efficiently solving a problem?</li>\n<li>How can different exploration methods be <strong>quantitatively evaluated</strong>? What are benefits and limitations of various metrics?</li>\n<li>How well do exploration methods <strong>generalize</strong> across environments? How can this generalization be measured?</li>\n<li>If exploration is posed as a learning problem (e.g., <strong>meta-learning</strong>), what should the learning objective be?</li>\n<li>Can exploration be cast as a problem of <strong>causal inference</strong>?</li>\n<li>What insight can be gained by casting exploration as <strong>unsupervised</strong> or <strong>semi-supervised</strong> learning?</li>\n<li>What exploration techniques are most effective in environments with very <strong>constrained environments</strong> (e.g., robots with physical constraints)?</li>\n<li>Do <strong>hierarchical approaches</strong> to exploration (e.g., with options) improve sample efficiency?</li>\n<li>Are certain exploration methods better suited to <strong>domain-specific applications</strong> (e.g., education, healthcare, robotics)?</li>\n<li>What insight can be gained by bridging the gap between reinforcement learning and <strong>bandits</strong>?</li>\n<li>What does exploration mean for <strong>evolutionary algorithms</strong>?</li>\n<li>What are the benefits of <strong>Bayesian</strong> <strong>exploration</strong> (e.g., safety, information gain)?</li>\n<li>Can <strong>ensembles</strong> of policies and/or value functions enable faster or safer exploration?</li>\n<li>What are the tradeoffs of including <strong>diversity objectives</strong> in exploration?</li>\n<li>Does <strong>safe exploration</strong> necessarily come at the cost of worse sample efficiency?</li>\n<li>How can exploration be down in a <strong>continual learning</strong> environment with <strong>no human supervision</strong> (i.e., no resets, no rewards)?</li>\n<li>Can <strong>auxiliary exploration objectives</strong> be cast in a unified framework?</li>\n<li>How can insights from <strong>intuitive physics</strong> and <strong>cognitive neuroscience</strong> improve exploration techniques?</li>\n<li>What insight can be gained by casting exploration as <strong>experimental design</strong>?</li>\n<li>What conceptual or theoretical frameworks might allow researchers to</li>\n</ol>\n<h3 id=\"reference\"><em>Reference</em><a title=\"#reference\" href=\"#reference\"></a></h3>\n<p>â€‹\t<a href=\"https://sites.google.com/view/erl-2019/open-problems\" target=\"_blank\">https://sites.google.com/view/erl-2019/open-problems</a></p>\n","prev":{"title":"Hypothesis Driven Exploration for Deep Reinforcement Learning","link":"2020/03/23/Hypothesis-Driven-Exploration-for-Deep-Reinforcement-Learning"},"next":{"title":"Inside theme+MathJax","link":"2020/03/17/Inside theme+MathJax"},"plink":"http://thswind.github.io/2020/03/21/RL-Open-problems/","toc":[{"id":"reference","title":"Reference","index":"1"}],"reading_time":"329 words in 2 min"}